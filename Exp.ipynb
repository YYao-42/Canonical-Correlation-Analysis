{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from numpy import linalg as LA\n",
    "from scipy import signal\n",
    "from scipy.linalg import toeplitz\n",
    "from scipy.stats import zscore, pearsonr\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from tqdm import tqdm\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data and pre-extracted features\n",
    "\n",
    "- Source of data: EEGVolume.mat -- Samantha Cohen's EEG data with 64 electrodes from 18 subjects (https://www.parralab.org/isc/EEGVolume.mat) while watching this video (https://www.parralab.org/videos/roccos_AV.mp4). The length of the video is 3m 17s.\n",
    "- EEG data is preprocessed and features of the video are extracted. Both with Dmochowski's code: https://github.com/dmochow/SRC\n",
    "- The features can be optical flow, contrast, luminance, and even sound envelope since in their experiment the sound is not muted. ('muFlow', 'muSqFlow', 'muTemporalContrast', 'muSqTemporalContrast', 'muLuminance', 'muSqLuminance', 'muLocalContrast', 'muSqLocalContrast', 'stdLocalContrast', 'soundEnvelope', 'soundEnvelopeDown') The features are averaged over all pixels, giving us a scalar for each frame.\n",
    "- Here we use optical flow because it gives a higher correlation (in this experiment). More specifically, we extract the average magnitude of the velocity in pixels per frame. Due to the Matlab CV toolbox version compatibility issue, the codes are rewritten a bit and the generated feature vector might be slightly different from the one generated by Dmochowski's original version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed EEG data\n",
    "EEG_data = scipy.io.loadmat('../Correlated Component Analysis/data/Prepro_EEG.mat')\n",
    "X_prepro = EEG_data['X']\n",
    "fsEEG = 256 # fs of EEG data\n",
    "T, D, N = X_prepro.shape # T=time/number of samples, D=number of channels, N=number of subjects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load video features\n",
    "features_data = scipy.io.loadmat('../Correlated Component Analysis/data/features.mat')\n",
    "fsStim = int(features_data['fsVideo']) # fs of the video \n",
    "features = np.nan_to_num(features_data['muFlow']) # feature: optical flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample EEG signals such that the sample frequency equals to the fs of the video\n",
    "downsampledEEG = signal.resample_poly(X_prepro, fsStim, fsEEG)\n",
    "features = features[:downsampledEEG.shape[0]]\n",
    "normalized_features = zscore(features) # normalize features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Only use the data of one subject and perform CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the EEG data of one subject\n",
    "EEG_of_one_sub = downsampledEEG[:,:,9]\n",
    "# Length of the time filter that will be applied on the extracted features. \n",
    "# It equals to the fs of the video, meaning that we take the video signals from one second ago to the current moment into consideration.\n",
    "L_timefilter = fsStim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 5\n",
    "# Find the convolution matrix and run CCA (with all data)\n",
    "conv_mtx = utils.convolution_mtx(L_timefilter, normalized_features)\n",
    "corr_coe, p_value, V_A, V_B = utils.cano_corr(EEG_of_one_sub, conv_mtx, n_components=n_components)\n",
    "corr_coe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_EEG = EEG_of_one_sub@V_A\n",
    "filtered_Sti = conv_mtx@V_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations of component 1\n",
    "plt.close()\n",
    "compo = 1\n",
    "times = np.array(range(len(features)))/fsStim\n",
    "ax1 = plt.subplot(411)\n",
    "ax1.plot(times, features)\n",
    "ax1.title.set_text('Features')\n",
    "ax2 = plt.subplot(412, sharex = ax1)\n",
    "ax2.plot(times, EEG_of_one_sub[:,0])\n",
    "ax2.title.set_text('EEG signals (of channel Fp1)')\n",
    "ax3 = plt.subplot(413, sharex = ax1)\n",
    "ax3.plot(times, filtered_Sti[:,compo-1])\n",
    "ax3.title.set_text('Filtered features (1st component)')\n",
    "ax4 = plt.subplot(414, sharex = ax1)\n",
    "ax4.plot(times, filtered_EEG[:,compo-1])\n",
    "ax4.title.set_text('Filtered EEG signals (1st component)')\n",
    "ax4.set_xlabel('time (s)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = times[1]-times[0]\n",
    "grad = np.gradient(np.squeeze(features), dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "ax1 = plt.subplot(211)\n",
    "ax1.plot(times, filtered_Sti[:,compo-1])\n",
    "ax1.title.set_text('Filtered features (1st component)')\n",
    "ax2 = plt.subplot(212, sharex = ax1)\n",
    "ax2.plot(times, grad)\n",
    "ax2.title.set_text('1st order derivative of the features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations of component 2\n",
    "plt.close()\n",
    "compo = 2\n",
    "ax1 = plt.subplot(411)\n",
    "ax1.plot(times, features)\n",
    "ax1.title.set_text('Features')\n",
    "ax2 = plt.subplot(412, sharex = ax1)\n",
    "ax2.plot(times, EEG_of_one_sub[:,0])\n",
    "ax2.title.set_text('EEG signals (of channel Fp1)')\n",
    "ax3 = plt.subplot(413, sharex = ax1)\n",
    "ax3.plot(times, filtered_Sti[:,compo-1])\n",
    "ax3.title.set_text('Filtered features (2nd component)')\n",
    "ax4 = plt.subplot(414, sharex = ax1)\n",
    "ax4.plot(times, filtered_EEG[:,compo-1])\n",
    "ax4.title.set_text('Filtered EEG signals (2nd component)')\n",
    "ax4.set_xlabel('time (s)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_grad = np.gradient(grad, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "ax1 = plt.subplot(211)\n",
    "ax1.plot(times, filtered_Sti[:,compo-1])\n",
    "ax1.title.set_text('Filtered features (2nd component)')\n",
    "ax2 = plt.subplot(212, sharex = ax1)\n",
    "ax2.plot(times, grad_grad)\n",
    "ax2.title.set_text('2nd order derivative of the features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equivalence of CCA and GCCA when there are only two datasets\n",
    "- A new function `GCCA_multi_modal` is added. It does not assume the input is pure EEG data of different subjects. The input can be a list of data of different modalities such as EEG data, video features, sound envelopes ... And it can also handle the case when the video feature is not a vector but a matrix, as long as the temporal/spatial/temporal-spatial filter is properly defined.\n",
    "- Here we verify that with only two datasets (EEG data of one subject and stimulus features), the results of CCA and GCCA are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = [EEG_of_one_sub, conv_mtx]\n",
    "_, W = utils.GCCA_multi_modal(datalist, n_components, regularization=None)\n",
    "W_EEG = W[:D,:]\n",
    "W_Stim = W[D:D+L_timefilter,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EEG_trans = EEG_of_one_sub@W_EEG\n",
    "Stim_trans = conv_mtx@W_Stim\n",
    "corr_pvalue = [pearsonr(EEG_trans[:,k], Stim_trans[:,k]) for k in range(n_components)]\n",
    "corr_coe = np.array([corr_pvalue[k][0] for k in range(n_components)])\n",
    "corr_coe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Validation\n",
    "\n",
    "10-fold cross-validation. Take the average of the results with different folds being test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 10\n",
    "corr_train = np.zeros((fold, n_components))\n",
    "corr_test = np.zeros((fold, n_components))\n",
    "for idx in range(fold):\n",
    "    EEG_train, EEG_test, Sti_train, Sti_test = utils.split(EEG_of_one_sub, normalized_features, fold=fold, fold_idx=idx+1)\n",
    "    conv_mtx_train = utils.convolution_mtx(L_timefilter, Sti_train)\n",
    "    corr_train[idx,:], p_value_train, V_A_train, V_B_train = utils.cano_corr(EEG_train, conv_mtx_train, n_components=n_components)\n",
    "    conv_mtx_test = utils.convolution_mtx(L_timefilter, Sti_test)\n",
    "    corr_test[idx,:], p_value_test, _, _ = utils.cano_corr(EEG_test, conv_mtx_test, n_components=n_components, V_A=V_A_train, V_B=V_B_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is quite dependent on which fold is the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(corr_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(corr_test, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make use of all data using GCCA\n",
    "\n",
    "Feed all the EEG data and the convolution matrix of the features into GCCA. Returned weight vector can be decomposed as the weights of spatial filters of different subjects, and the weights of the time filter of extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = [downsampledEEG, conv_mtx]\n",
    "_, W = utils.GCCA_multi_modal(datalist, n_components, regularization='lwcov')\n",
    "W_EEG = W[:N*D,:]\n",
    "W_EEG_stack = np.reshape(W_EEG, (N,D,-1))\n",
    "W_EEG_stack = np.transpose(W_EEG_stack, [1,0,2]) # W: D*N*n_components\n",
    "W_Stim = W[N*D:N*D+L_timefilter,:]\n",
    "Wlist = [W_EEG_stack, W_Stim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_corr = utils.avg_corr_coe_multi_modal(datalist, Wlist, n_components=5)\n",
    "avg_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 10\n",
    "cv_n_components = 5\n",
    "corr_train = np.zeros((fold, cv_n_components))\n",
    "corr_test = np.zeros_like(corr_train)\n",
    "for i in range(fold):\n",
    "    EEG_train, EEG_test, Sti_train, Sti_test = utils.split(downsampledEEG, normalized_features, fold=fold, fold_idx=i+1)\n",
    "    _, W_train = utils.GCCA_multi_modal([EEG_train, Sti_train], cv_n_components, regularization='lwcov')\n",
    "    W_EEG_train = W_train[:N*D,:]\n",
    "    W_EEG_stack_train = np.reshape(W_EEG_train, (N,D,-1))\n",
    "    W_EEG_stack_train = np.transpose(W_EEG_stack_train, [1,0,2]) # W: D*N*n_components\n",
    "    W_Stim_train = W_train[N*D:N*D+L_timefilter,:]\n",
    "    Wlist_train = [W_EEG_stack_train, W_Stim_train]\n",
    "    corr_train[i,:] = utils.avg_corr_coe_multi_modal([EEG_train, Sti_train], Wlist_train, n_components=5)\n",
    "    corr_test[i,:] = utils.avg_corr_coe_multi_modal([EEG_test, Sti_test], Wlist_train, n_components=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(corr_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(corr_test, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can observe a large drop of correlation coefficients for unseen data, indicating the amount of data is insufficient. (The length of data is 3m 17s, and it is downsampled to 30 Hz.)\n",
    "- The correlation of the first component of the last fold is very low (8.29320096e-05). It probably because the last 15 seconds of the video is all black. Then we are basically finding the correlation of the background EEG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we do not include features\n",
    "\n",
    "- The result could be an indication of the quality of the features. Good features may guide the algorithm to do better EEG enhancement and thus lead to higher correlation. Bad features will drag the correlation down.\n",
    "- Also if features are not included, then we can compare the results with the ones when EEG data is not downsampled and see how downsampling will affect the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 10\n",
    "corr_train = np.zeros((fold, cv_n_components))\n",
    "corr_test = np.zeros_like(corr_train)\n",
    "X_downsampled = downsampledEEG\n",
    "T, D, N = X_downsampled.shape\n",
    "for i in range(fold):\n",
    "    len_test = T // fold\n",
    "    X_test = X_downsampled[len_test*i:len_test*(i+1),:,:]\n",
    "    X_train = np.delete(X_downsampled, range(len_test*i, len_test*(i+1)), axis=0)\n",
    "    _, W_train, _ = utils.GCCA(X_train, n_components=n_components, regularization='lwcov')\n",
    "    corr_train[i,:] = utils.avg_corr_coe(X_train, W_train, N, n_components=n_components)\n",
    "    corr_test[i,:] = utils.avg_corr_coe(X_test, W_train, N, n_components=n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(corr_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(corr_test, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Slightly higher than including stimulus. \n",
    "- For reference, the results when EEG is not downsampled are:\n",
    "\n",
    "    Training set: array([0.12882401, 0.12098105, 0.11829638, 0.11620297, 0.11211726])\n",
    "\n",
    "    Test set: array([0.01916707, 0.00462971, 0.0070183 , 0.01117539, 0.00070067])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do not include features and compare GCCA with correlated component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 10\n",
    "ISC_train = np.zeros((fold, cv_n_components))\n",
    "ISC_test = np.zeros_like(ISC_train)\n",
    "X_downsampled = downsampledEEG\n",
    "T, D, N = X_downsampled.shape\n",
    "for i in range(fold):\n",
    "    len_test = T // fold\n",
    "    X_test = X_downsampled[len_test*i:len_test*(i+1),:,:]\n",
    "    X_train = np.delete(X_downsampled, range(len_test*i, len_test*(i+1)), axis=0)\n",
    "    ISC_train[i,:], W_train = utils.corr_component(X_train, n_components=n_components)\n",
    "    ISC_test[i,:], _ = utils.corr_component(X_test, n_components=n_components, W_train=W_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(ISC_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(ISC_test, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The overfitting problem is suppressed compared to GCCA. And the performance on the test set is even better.\n",
    "- For reference, the results when EEG is not downsampled are:\n",
    "\n",
    "    Training set: array([0.05022358, 0.02988394, 0.02490273, 0.01870672, 0.01760152])\n",
    "\n",
    "    Test set: array([0.04280363,  0.02148039,  0.01509772, -0.00111595,  0.00240869])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In both GCCA and correlated component analysis, results of downsampled data are better. Can't draw any conclusion yet with this small amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4780ce94013b4ad826834d504b051d615119766f3ac7f8bac99efc1ee879921"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
